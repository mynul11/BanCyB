{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully for seeds: [0, 42, 123]\n",
      "Number of samples: 763\n"
     ]
    }
   ],
   "source": [
    "# Load data from all seeds\n",
    "seed_files = {\n",
    "    0: \"true_pred_only_seed(0).csv\",\n",
    "    42: \"true_pred_only_seed(42).csv\",\n",
    "    123: \"true_pred_only_seed(123).csv\"\n",
    "}\n",
    "\n",
    "data_seeds = {}\n",
    "for seed, filename in seed_files.items():\n",
    "    data_seeds[seed] = pd.read_csv(filename)\n",
    "\n",
    "print(\"Data loaded successfully for seeds:\", list(data_seeds.keys()))\n",
    "print(f\"Number of samples: {len(data_seeds[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c9c5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores for each seed:\n",
      "\n",
      "Label                     Seed 0       Seed 42      Seed 123     Mean         Std         \n",
      "=====================================================================================\n",
      "Trolling                  0.8169       0.8221       0.8013       0.8134       0.0089      \n",
      "Insult                    0.9220       0.9197       0.9319       0.9245       0.0053      \n",
      "Hate Speech               0.8567       0.8507       0.8584       0.8553       0.0033      \n",
      "Targeted Harassment       0.8801       0.8895       0.8816       0.8837       0.0041      \n",
      "\n",
      "Macro-F1 for seed 0: 0.8689\n",
      "\n",
      "Macro-F1 for seed 42: 0.8705\n",
      "\n",
      "Macro-F1 for seed 123: 0.8683\n",
      "\n",
      "Overall Mean Macro-F1: 0.8692 ± 0.0009\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 scores for each class and each seed\n",
    "labels = ['Trolling', 'Insult', 'Hate Speech', 'Targeted Harassment']\n",
    "\n",
    "# Store F1 scores: {label: [seed0_f1, seed42_f1, seed123_f1]}\n",
    "f1_scores = {label: [] for label in labels}\n",
    "\n",
    "for seed in [0, 42, 123]:\n",
    "    df = data_seeds[seed]\n",
    "    for label in labels:\n",
    "        true_col = label\n",
    "        pred_col = f'Pred_{label.replace(\" \", \"_\")}'\n",
    "        \n",
    "        f1 = f1_score(df[true_col], df[pred_col], average='binary')\n",
    "        f1_scores[label].append(f1)\n",
    "\n",
    "# Display F1 scores\n",
    "print(\"F1 Scores for each seed:\\n\")\n",
    "print(f\"{'Label':<25} {'Seed 0':<12} {'Seed 42':<12} {'Seed 123':<12} {'Mean':<12} {'Std':<12}\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "for label in labels:\n",
    "    scores = f1_scores[label]\n",
    "    print(f\"{label:<25} {scores[0]:<12.4f} {scores[1]:<12.4f} {scores[2]:<12.4f} {np.mean(scores):<12.4f} {np.std(scores):<12.4f}\")\n",
    "\n",
    "# Calculate Macro-F1 for each seed\n",
    "macro_f1_per_seed = []\n",
    "for i, seed in enumerate([0, 42, 123]):\n",
    "    seed_f1s = [f1_scores[label][i] for label in labels]\n",
    "    macro_f1 = np.mean(seed_f1s)\n",
    "    macro_f1_per_seed.append(macro_f1)\n",
    "    print(f\"\\nMacro-F1 for seed {seed}: {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Mean Macro-F1: {np.mean(macro_f1_per_seed):.4f} ± {np.std(macro_f1_per_seed):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "831687c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STATISTICAL SIGNIFICANCE TESTING\n",
      "================================================================================\n",
      "\n",
      "1. Comparing Macro-F1 scores across different seeds:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Note: With only 3 samples, normality testing is limited.\n",
      "We'll use Wilcoxon signed-rank test (non-parametric) for robustness.\n",
      "\n",
      "Seed 0 vs Seed 42:\n",
      "  Mean F1 difference: -0.0016\n",
      "  Wilcoxon test statistic: 4.0000\n",
      "  p-value: 0.8750\n",
      "  Significant (p < 0.05): NO\n",
      "\n",
      "Seed 0 vs Seed 123:\n",
      "  Mean F1 difference: +0.0006\n",
      "  Wilcoxon test statistic: 4.0000\n",
      "  p-value: 0.8750\n",
      "  Significant (p < 0.05): NO\n",
      "\n",
      "Seed 42 vs Seed 123:\n",
      "  Mean F1 difference: +0.0022\n",
      "  Wilcoxon test statistic: 4.0000\n",
      "  p-value: 0.8750\n",
      "  Significant (p < 0.05): NO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical Significance Testing\n",
    "# We'll test if there are significant differences between seeds\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For paired tests, we need to compare seeds pairwise\n",
    "seed_pairs = [(0, 42), (0, 123), (42, 123)]\n",
    "\n",
    "# Method 1: Compare macro-F1 scores across seeds\n",
    "print(\"\\n1. Comparing Macro-F1 scores across different seeds:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check normality of macro-F1 differences\n",
    "print(\"\\nNote: With only 3 samples, normality testing is limited.\")\n",
    "print(\"We'll use Wilcoxon signed-rank test (non-parametric) for robustness.\\n\")\n",
    "\n",
    "for seed1, seed2 in seed_pairs:\n",
    "    idx1 = [0, 42, 123].index(seed1)\n",
    "    idx2 = [0, 42, 123].index(seed2)\n",
    "    \n",
    "    # Get F1 scores for each label from both seeds\n",
    "    scores_seed1 = [f1_scores[label][idx1] for label in labels]\n",
    "    scores_seed2 = [f1_scores[label][idx2] for label in labels]\n",
    "    \n",
    "    # Perform Wilcoxon signed-rank test (paired, non-parametric)\n",
    "    stat, p_value = wilcoxon(scores_seed1, scores_seed2)\n",
    "    \n",
    "    mean_diff = np.mean(scores_seed1) - np.mean(scores_seed2)\n",
    "    \n",
    "    print(f\"Seed {seed1} vs Seed {seed2}:\")\n",
    "    print(f\"  Mean F1 difference: {mean_diff:+.4f}\")\n",
    "    print(f\"  Wilcoxon test statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant (p < 0.05): {'YES' if p_value < 0.05 else 'NO'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1479ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Comparing per-instance macro-F1 scores:\n",
      "--------------------------------------------------------------------------------\n",
      "Per-instance macro-F1 statistics:\n",
      "\n",
      "Seed 0: Mean=0.8539, Std=0.1965\n",
      "Seed 42: Mean=0.8578, Std=0.1927\n",
      "Seed 123: Mean=0.8565, Std=0.1930\n",
      "\n",
      "\n",
      "Paired statistical tests (per-instance comparison):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Seed 0 vs Seed 42:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: -0.0039\n",
      "  Test statistic: 7499.0000\n",
      "  p-value: 0.4494\n",
      "  Significant (p < 0.05): NO ✗\n",
      "\n",
      "Seed 0 vs Seed 123:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: -0.0026\n",
      "  Test statistic: 6247.0000\n",
      "  p-value: 0.6100\n",
      "  Significant (p < 0.05): NO ✗\n",
      "\n",
      "Seed 42 vs Seed 123:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: +0.0013\n",
      "  Test statistic: 5892.5000\n",
      "  p-value: 0.7609\n",
      "  Significant (p < 0.05): NO ✗\n",
      "Per-instance macro-F1 statistics:\n",
      "\n",
      "Seed 0: Mean=0.8539, Std=0.1965\n",
      "Seed 42: Mean=0.8578, Std=0.1927\n",
      "Seed 123: Mean=0.8565, Std=0.1930\n",
      "\n",
      "\n",
      "Paired statistical tests (per-instance comparison):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Seed 0 vs Seed 42:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: -0.0039\n",
      "  Test statistic: 7499.0000\n",
      "  p-value: 0.4494\n",
      "  Significant (p < 0.05): NO ✗\n",
      "\n",
      "Seed 0 vs Seed 123:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: -0.0026\n",
      "  Test statistic: 6247.0000\n",
      "  p-value: 0.6100\n",
      "  Significant (p < 0.05): NO ✗\n",
      "\n",
      "Seed 42 vs Seed 123:\n",
      "  Test used: Wilcoxon signed-rank test\n",
      "  Mean difference: +0.0013\n",
      "  Test statistic: 5892.5000\n",
      "  p-value: 0.7609\n",
      "  Significant (p < 0.05): NO ✗\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Per-instance predictions to compare overall model stability\n",
    "# Calculate per-instance F1 across all labels for each seed\n",
    "\n",
    "print(\"\\n2. Comparing per-instance macro-F1 scores:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate macro-F1 for each instance across all labels\n",
    "def calculate_instance_macro_f1(df, labels):\n",
    "    \"\"\"Calculate macro-F1 for each test instance\"\"\"\n",
    "    instance_f1s = []\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        f1s = []\n",
    "        for label in labels:\n",
    "            true_col = label\n",
    "            pred_col = f'Pred_{label.replace(\" \", \"_\")}'\n",
    "            \n",
    "            true_val = df.iloc[idx][true_col]\n",
    "            pred_val = df.iloc[idx][pred_col]\n",
    "            \n",
    "            # F1 for single instance (binary: correct=1, incorrect=0)\n",
    "            if true_val == pred_val:\n",
    "                f1s.append(1.0)\n",
    "            else:\n",
    "                f1s.append(0.0)\n",
    "        \n",
    "        instance_f1s.append(np.mean(f1s))\n",
    "    \n",
    "    return np.array(instance_f1s)\n",
    "\n",
    "# Get per-instance scores for each seed\n",
    "instance_scores = {}\n",
    "for seed in [0, 42, 123]:\n",
    "    instance_scores[seed] = calculate_instance_macro_f1(data_seeds[seed], labels)\n",
    "\n",
    "print(f\"Per-instance macro-F1 statistics:\\n\")\n",
    "for seed in [0, 42, 123]:\n",
    "    scores = instance_scores[seed]\n",
    "    print(f\"Seed {seed}: Mean={np.mean(scores):.4f}, Std={np.std(scores):.4f}\")\n",
    "\n",
    "# Perform paired tests on per-instance scores\n",
    "print(\"\\n\\nPaired statistical tests (per-instance comparison):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for seed1, seed2 in seed_pairs:\n",
    "    scores1 = instance_scores[seed1]\n",
    "    scores2 = instance_scores[seed2]\n",
    "    \n",
    "    # Check normality\n",
    "    _, p_norm1 = shapiro(scores1[:50])  # Use sample for Shapiro test\n",
    "    _, p_norm2 = shapiro(scores2[:50])\n",
    "    \n",
    "    if p_norm1 > 0.05 and p_norm2 > 0.05:\n",
    "        # Data appears normal, use paired t-test\n",
    "        stat, p_value = ttest_rel(scores1, scores2)\n",
    "        test_name = \"Paired t-test\"\n",
    "    else:\n",
    "        # Data not normal, use Wilcoxon\n",
    "        stat, p_value = wilcoxon(scores1, scores2)\n",
    "        test_name = \"Wilcoxon signed-rank test\"\n",
    "    \n",
    "    mean_diff = np.mean(scores1) - np.mean(scores2)\n",
    "    \n",
    "    print(f\"\\nSeed {seed1} vs Seed {seed2}:\")\n",
    "    print(f\"  Test used: {test_name}\")\n",
    "    print(f\"  Mean difference: {mean_diff:+.4f}\")\n",
    "    print(f\"  Test statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant (p < 0.05): {'YES ✓' if p_value < 0.05 else 'NO ✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
